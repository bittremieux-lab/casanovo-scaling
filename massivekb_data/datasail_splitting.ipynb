{
 "cells": [
  {
   "cell_type": "code",
   "id": "c88eb02d12d94b4b",
   "metadata": {},
   "source": [
    "# Create a subset of the mgf file for testing\n",
    "with open(\"./massivekb_82c0124b.mgf\", 'r') as infile, open(\"./massivekb_subset.mgf\", 'w') as outfile:\n",
    "    for i, line in enumerate(infile):\n",
    "        if i >= 2e7:\n",
    "            break\n",
    "        outfile.write(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import re\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from pyteomics import mgf\n",
    "from tqdm import tqdm\n",
    "\n",
    "massivekb_file = \"./massivekb_82c0124b.mgf\"\n",
    "\n",
    "CAPS_ONLY = re.compile(r'[^A-Z]')\n",
    "\n",
    "\n",
    "def remove_ptms(seq):\n",
    "    return CAPS_ONLY.sub('', seq)\n",
    "\n",
    "\n",
    "t = time()\n",
    "nones = 0\n",
    "seqs = []\n",
    "with mgf.read(massivekb_file, use_index=False, convert_arrays=0, read_charges=False, read_ions=False, ) as massivekb:\n",
    "    for i, spectrum in enumerate(tqdm(massivekb, total=None)):\n",
    "        try:\n",
    "            seqs.append(remove_ptms(spectrum['params']['seq']))\n",
    "        except TypeError as e:\n",
    "            if spectrum is None:\n",
    "                nones += 1\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "print(f\"Reading {len(seqs)} sequences from {massivekb_file} took {(time() - t) / 60:.2f} minutes\")\n",
    "print(f\"{nones} invalid spectra\")\n",
    "print(f\"{len(set(seqs))} unique sequences\")\n",
    "\n",
    "np.save(\"massivekb_82c0124b_seqs.npy\", set(seqs), allow_pickle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a035272-3b8e-41f2-85b6-3c856d5dfbdc",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the set of unique peptide sequences as a list\n",
    "seqs = list(np.load(\"massivekb_82c0124b_seqs.npy\", allow_pickle=True).item())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e85a8d5-145e-4903-8c27-34e648f74a68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import Levenshtein\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "n_datasail = 100000\n",
    "norm_method = \"max\"\n",
    "random_seqs = random.sample(seqs, n_datasail)\n",
    "\n",
    "dist_matrix = np.zeros((n_datasail, n_datasail), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def compute_row(i):\n",
    "    row = np.zeros(n_datasail - i - 1, dtype=np.uint8)  # only store distances for j > i\n",
    "    for offset, j in enumerate(range(i + 1, n_datasail)):\n",
    "        d = Levenshtein.distance(random_seqs[i], random_seqs[j])\n",
    "        if norm_method == \"max\":\n",
    "            d /= max(len(random_seqs[i]), len(random_seqs[j]))\n",
    "            d *= 100\n",
    "        row[offset] = d\n",
    "    return i, row\n",
    "\n",
    "\n",
    "with Pool(cpu_count()) as pool:\n",
    "    # Parallelize over rows i\n",
    "    for i, row in tqdm(pool.imap_unordered(compute_row, range(n_datasail - 1)), total=n_datasail - 1):\n",
    "        dist_matrix[i, i + 1:] = row\n",
    "        dist_matrix[i + 1:, i] = row\n",
    "\n",
    "np.save(f\"massivekb_dist_matrix_{norm_method}_{n_datasail}.npy\", dist_matrix)\n",
    "np.save(f\"massivekb_seqs_{n_datasail}.npy\", random_seqs, allow_pickle=True)\n",
    "dist_matrix\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c6c300dcfad7805",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasail.sail import datasail\n",
    "\n",
    "n_datasail = 100000\n",
    "norm_method = \"max\"\n",
    "\n",
    "dist_matrix = np.load(f\"massivekb_dist_matrix_{norm_method}_{n_datasail}.npy\")\n",
    "random_seqs = np.load(f\"massivekb_seqs_{n_datasail}.npy\", allow_pickle=True)\n",
    "\n",
    "e_splits, _, _ = datasail(\n",
    "    verbose='D',\n",
    "    techniques=[\"C1e\"],\n",
    "    splits=[.84, .08, .08],\n",
    "    names=[\"train\", \"val\", \"test\"],\n",
    "    runs=1,\n",
    "    e_type=\"O\",\n",
    "    epsilon=0.1,\n",
    "    e_clusters=200,\n",
    "    e_data={i: s for i, s in enumerate(random_seqs)},\n",
    "    e_dist=(list(range(n_datasail)), dist_matrix),\n",
    "    threads=64,\n",
    ")\n",
    "\n",
    "df = pd.DataFrame.from_dict(e_splits['C1e'][0], orient='index', columns=['split'])\n",
    "df['seq'] = random_seqs\n",
    "df.to_csv(f\"massivekb_split_df_{norm_method}_{n_datasail}.csv\")\n",
    "\n",
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "test_df = df[df['split'] == 'test']\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# Global variables for use in the worker function\n",
    "_global_seqs1 = None\n",
    "_global_seqs2 = None\n",
    "\n",
    "\n",
    "def _init_pool(seqs1, seqs2):\n",
    "    global _global_seqs1, _global_seqs2\n",
    "    _global_seqs1 = seqs1\n",
    "    _global_seqs2 = seqs2\n",
    "\n",
    "\n",
    "def compute_row(i):\n",
    "    s1 = _global_seqs1[i]\n",
    "    row = np.zeros(len(_global_seqs2), dtype=np.uint8)\n",
    "    for j, s2 in enumerate(_global_seqs2):\n",
    "        row[j] = levenshtein_distance(s1, s2)\n",
    "    return (i, row)\n",
    "\n",
    "\n",
    "def compute_distance_matrix(seqs1, seqs2):\n",
    "    \"\"\"\n",
    "    Compute Levenshtein distance matrix between two lists of sequences in parallel.\n",
    "\n",
    "    Args:\n",
    "        seqs1 (List[str]): First list of sequences (rows).\n",
    "        seqs2 (List[str]): Second list of sequences (columns).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Distance matrix of shape (len(seqs1), len(seqs2)) with dtype=uint8.\n",
    "    \"\"\"\n",
    "    n1 = len(seqs1)\n",
    "    n2 = len(seqs2)\n",
    "    dist_matrix = np.zeros((n1, n2), dtype=np.uint8)\n",
    "\n",
    "    # Create the pool and initialize globals\n",
    "    with Pool(cpu_count(), initializer=_init_pool, initargs=(seqs1, seqs2)) as pool:\n",
    "        for i, row in tqdm(pool.imap_unordered(compute_row, range(n1)), total=n1):\n",
    "            dist_matrix[i, :] = row\n",
    "\n",
    "    return dist_matrix"
   ],
   "id": "cd6b620fefe325a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_datasail = 100000\n",
    "norm_method = \"max\"\n",
    "\n",
    "df = pd.read_csv(f\"massivekb_split_df_{norm_method}_{n_datasail}.csv\")\n",
    "\n",
    "train_df = df[df['split'] == 'train']\n",
    "val_df = df[df['split'] == 'val']\n",
    "test_df = df[df['split'] == 'test']\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))\n",
    "\n",
    "train_lengths = train_df['seq'].str.len()\n",
    "plt.hist(train_lengths)\n",
    "plt.show()\n",
    "\n",
    "val_lengths = val_df['seq'].str.len()\n",
    "plt.hist(val_lengths)\n",
    "plt.show()\n",
    "\n",
    "test_lengths = test_df['seq'].str.len()\n",
    "plt.hist(test_lengths)\n",
    "plt.show()\n",
    "\n",
    "all_seqs = list(np.load(\"massivekb_82c0124b_seqs.npy\", allow_pickle=True).item())\n",
    "\n",
    "dist_matrix = compute_distance_matrix(all_seqs, df['seq'].values)\n",
    "nearest_indices = np.argmin(dist_matrix, axis=1)\n",
    "split_labels = df['split'].values[nearest_indices]\n",
    "\n",
    "split_df = pd.DataFrame({'seq': all_seqs, 'split': split_labels})\n",
    "split_df.to_csv(f\"massivekb_full_split_df_{norm_method}_{n_datasail}.csv\")\n"
   ],
   "id": "77da17db1a2460d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "n_datasail = 100000\n",
    "norm_method = \"max\"\n",
    "\n",
    "split_df = pd.read_csv(f\"massivekb_full_split_df_{norm_method}_{n_datasail}.csv\")\n",
    "\n",
    "train_df = split_df[split_df['split'] == 'train']\n",
    "val_df = split_df[split_df['split'] == 'val']\n",
    "test_df = split_df[split_df['split'] == 'test']\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))\n",
    "\n",
    "train_lengths = train_df['seq'].str.len()\n",
    "plt.hist(train_lengths)\n",
    "plt.show()\n",
    "\n",
    "val_lengths = val_df['seq'].str.len()\n",
    "plt.hist(val_lengths)\n",
    "plt.show()\n",
    "\n",
    "test_lengths = test_df['seq'].str.len()\n",
    "plt.hist(test_lengths)\n",
    "plt.show()"
   ],
   "id": "cdf7ea1547aca3dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from pyteomics import mgf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def remove_ptms(seq):\n",
    "    return ''.join([c for c in seq if 'A' <= c <= 'Z'])\n",
    "\n",
    "\n",
    "n_datasail = 100000\n",
    "norm_method = \"max\"\n",
    "\n",
    "n_train_peps = [int(i) for i in [1e5, 2.5e5, 5e5, 7.5e5, 1e6]]\n",
    "# n_train_peps = [int(i) for i in [1e3, 1e4, 1e5]]\n",
    "\n",
    "n_train_spectra = [1, 2, 5, 10, 20]\n",
    "\n",
    "massivekb_dir = \"/mnt/data/cdens/casanovo-scaling/massivekb_data/\"\n",
    "massivekb_file = os.path.join(massivekb_dir, \"massivekb_82c0124b.mgf\")\n",
    "\n",
    "scaling_dir = os.path.join(massivekb_dir, f\"scaling_data_{norm_method}_{n_datasail}\")\n",
    "os.makedirs(scaling_dir, exist_ok=True)\n",
    "\n",
    "split_df = pd.read_csv(f\"massivekb_full_split_df_{norm_method}_{n_datasail}.csv\")\n",
    "split_dict = dict(zip(split_df['seq'], split_df['split']))  # convert to dict for more efficient lookup\n",
    "\n",
    "spectra = {k: [] for k in ['train', 'val', 'test']}\n",
    "train_index = defaultdict(list)\n",
    "with mgf.read(massivekb_file, use_index=False, convert_arrays=0, read_charges=False, read_ions=False) as massivekb:\n",
    "    for spectrum in tqdm(massivekb, total=3e7):\n",
    "        unmod_pep = remove_ptms(spectrum['params']['seq'])\n",
    "        split = split_dict[unmod_pep]\n",
    "        spectra[split].append(spectrum)\n",
    "\n",
    "        # Creating an index of the spectra per modified peptide in the train dataset\n",
    "        if split == 'train':\n",
    "            train_index[spectrum['params']['seq']].append(len(spectra['train']) - 1)\n",
    "\n",
    "mgf.write(spectra['val'], os.path.join(scaling_dir, f\"val.mgf\"))\n",
    "mgf.write(spectra['test'], os.path.join(scaling_dir, f\"test.mgf\"))\n",
    "\n",
    "# SOME PTM STATS FROM THE TRAIN DATASET\n",
    "extracted = []\n",
    "for seq in train_index.keys():\n",
    "    matches = re.findall(r'([A-Z]?)([+-]?\\d+\\.\\d+)', seq)\n",
    "    for residue, ptm in matches:\n",
    "        if residue == \"\":\n",
    "            residue = \"N-term\"  # fillna equivalent\n",
    "        extracted.append((residue, ptm))\n",
    "mod_count = Counter(extracted)\n",
    "print(\"PTM counts:\")\n",
    "print(mod_count)\n",
    "\n",
    "spectra_count_dict = {p: len(l) for p, l in train_index.items()}\n",
    "print(f\"Number of modified peptides: {len(spectra_count_dict)}\")\n",
    "plt.hist(spectra_count_dict.values(),\n",
    "         bins=np.arange(min(spectra_count_dict.values()) - 0.5, max(spectra_count_dict.values()) + 0.5, 1))\n",
    "plt.xlabel(\"Number of spectra\")\n",
    "plt.ylabel(\"Number of peptides\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "for n_train_s in n_train_spectra:\n",
    "    # Get all peptides with enough spectra\n",
    "    p_with_enough_spectra = [p for p, c in spectra_count_dict.items() if c >= n_train_s]\n",
    "    remaining_peptides = [p for p, c in spectra_count_dict.items() if c < n_train_s]\n",
    "\n",
    "    print(f\"There are {len(p_with_enough_spectra)} peptides with at least {n_train_s} spectra\")\n",
    "\n",
    "    for n_train_p in n_train_peps:\n",
    "        print(f\"Getting {n_train_s} spectra for {n_train_p} peptides\")\n",
    "        if len(spectra_count_dict.keys()) < n_train_p:\n",
    "            train_spectra = []\n",
    "            print(f\"There are only {len(spectra_count_dict.keys())} peptides but requested {n_train_p}\")\n",
    "\n",
    "        elif len(p_with_enough_spectra) >= n_train_p:\n",
    "            # Sample random peptides and spectra\n",
    "            train_peptides = random.sample(p_with_enough_spectra, n_train_p)\n",
    "            train_spectra = [spec_i for train_pep in train_peptides for spec_i in\n",
    "                             random.sample(train_index[train_pep], n_train_s)]\n",
    "            print(f\"Sufficient peptides with sufficient spectra, number of spectra selected: {len(train_spectra)}\")\n",
    "        else:\n",
    "            # Get all peptides with enough spectra and sample random spectra\n",
    "            train_spectra = [spec_i for train_pep in p_with_enough_spectra for spec_i in\n",
    "                             random.sample(train_index[train_pep], n_train_s)]\n",
    "            print(f\"Got {len(train_spectra)} spectra from peptides with enough spectra\")\n",
    "\n",
    "            # Count how many are missing\n",
    "            n_train_p_to_add = n_train_p - len(p_with_enough_spectra)\n",
    "\n",
    "            # Select random peptides to add\n",
    "            train_p_to_add = random.sample(remaining_peptides, n_train_p_to_add)\n",
    "\n",
    "            # Add all spectra for these random peptides\n",
    "            train_spectra += [spec_i for train_pep in train_p_to_add for spec_i in train_index[train_pep]]\n",
    "            print(f\"Added all spectra from random peptides, selected {len(train_spectra)} spectra in total now\")\n",
    "\n",
    "        # Shuffle, then get spectra from indices and write to mgf\n",
    "        random.shuffle(train_spectra)\n",
    "        train_spectra = [spectra['train'][i] for i in train_spectra]\n",
    "        mgf.write(train_spectra, os.path.join(scaling_dir, f\"train_{n_train_s}s_{n_train_p}p.mgf\"))\n",
    "        print()"
   ],
   "id": "8ed4ac475110857d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "985f30231b63136c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a random subset of the validation data",
   "id": "24ae75ecc6576ea5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T12:28:26.343093Z",
     "start_time": "2025-08-21T12:22:08.921302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyteomics import mgf\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_fraction(iter, frac):\n",
    "    for item in iter:\n",
    "        if random.random() < frac:  # 25% chance\n",
    "            yield item\n",
    "\n",
    "\n",
    "val_file = \"scaling_data_max_100000/val.mgf\"\n",
    "fraction = 0.25\n",
    "out_file = os.path.join(\"scaling_data_max_100000\", f\"val_{fraction}.mgf\")\n",
    "with mgf.read(val_file, use_index=False, convert_arrays=0, read_charges=False, read_ions=False, ) as val:\n",
    "    mgf.write(tqdm(sample_fraction(val, fraction), total=2e6), out_file)\n",
    "\n",
    "\n"
   ],
   "id": "1d241d1841fbf43e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 517938/2000000.0 [06:17<17:59, 1372.36it/s] \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create a subset of the training data for batch size determining",
   "id": "56e8ab67da3d1894"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T14:08:22.768421Z",
     "start_time": "2025-08-29T14:08:08.848216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyteomics import mgf\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_fraction(iter, frac):\n",
    "    for item in iter:\n",
    "        if random.random() < frac:  # 25% chance\n",
    "            yield item\n",
    "\n",
    "\n",
    "file = \"scaling_data_max_100000/train_1s_100000p.mgf\"\n",
    "fraction = 0.2\n",
    "out_file = os.path.join(\"scaling_data_max_100000\", f\"bs_finder.mgf\")\n",
    "with mgf.read(file, use_index=False, convert_arrays=0, read_charges=False, read_ions=False, ) as f:\n",
    "    mgf.write(sample_fraction(f, fraction), out_file)"
   ],
   "id": "db0b425d28c1627",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
